{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List your imports here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High concentrations of certain bacteria beaches lagoons such as in Wakiki or Ko'olina, can be a serious health concern for beachgoers and tourists frequenting those beach lagoons.  Monitoring and predicting high-level of bacteria is essential to improving assuring their safety and mitigating the hazards that cause them. Current methods for assessing the levels of bacteria are time-consuming and require growing the bacteria for 24 hours before testing the samples.\n",
    "\n",
    "Here, you will build a model which uses chemical monitoring, an inexpensive and easily automated approach to\n",
    "predict the concentrations of 4 different types of bacteria (B1, B2, B3, and B4) in beach lagoons. Specifically, your model will provide a faster way to assess the levels of the four harmful bacteria based on chemical and physical measurements that are fast (run in less than 2 hours) and easy to carry out. \n",
    "\n",
    "Your data consists of several lagoon water samples collected in one year. These measurements include chemical and physical measurements, as well as levels of the four bacteria we are interested in predicting in out-of-sample data. The data is stored in the file `data/bacteria_final.csv`\n",
    "\n",
    "\n",
    "The dataset contains the following data:  \n",
    "- Season of the year when the water samples were collected (summer, spring, autumn, winter)\n",
    "- Wave action outside of the lagoon (small, medium or large)\n",
    "- Water current condition (low, medium or high)\n",
    "- Maximum pH value\n",
    "- Water oxygen level\n",
    "- Chloride Concentration\n",
    "- Nitrates Concentration\n",
    "- CO2 Concentration\n",
    "- Abundances of Bacteria B1-B4 (four last columns). Those are the target values you will be modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- How many entries does that table contain?\n",
    "- How many features does this dataset have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "bacteria = pd.read_csv('data/bacteria_final.csv')\n",
    "bacteria.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main goal of our this study is to predict the frequencies of the four bacteria for out-of-sample data. Which type of learning task is this?\n",
    "  1. Supervised or unsupervised?\n",
    "  2. Regression or classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your answer here\n",
    "Supervised, regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Errors in the Data\n",
    "\n",
    "Inspect your data to make sure it's dot not contain any inconsistencies. \n",
    "\n",
    "Hint: Often, categorical attributes are entered manually and are, therefore, subject to human error or inconsistencies. Use your judgment to fix inconsistencies, if any.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "bacteria.replace(regex='_', value='', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python expression to show that your resulting data frame does not contain the inconsistencies identified in the original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your answer here\n",
    "not bacteria['wave'].str.contains('_').any() and not bacteria['current'].str.contains('_').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the categorical fearures \n",
    "\n",
    "Encode the categorical variables using a proper encoding. You can do this using python code using `sklern`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\n",
    "label_encoder = LabelEncoder()\n",
    "bacteria['season'] = label_encoder.fit_transform(bacteria['season'])\n",
    "bacteria['wave'] = label_encoder.fit_transform(bacteria['wave'])\n",
    "bacteria['current'] = label_encoder.fit_transform(bacteria['current'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing features\n",
    "\n",
    "-  Many machine learning algorithms cannot handle missing features. While it's acceptable to discard a few instances in missing values when you workign with large data sets, it's preferable to infer (impute) missing values when working relatively small datasets. Here you will use regression to impute missing values using the approach described below. Given the following dataset  with a missing value in the column `C`:\n",
    "\n",
    "<img src=\"images/ex_missing_values.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "\n",
    "Computing the correlation between columns C and A shows that both columns are very highly correlated ($r \\approx 0.99$). We could use that information to predict the missing values in C by predicting them using a linear regression between both columns. For instance, according to the graph below, we see that when A is 1.9, C is ~ 3.7. \n",
    "\n",
    "<img src=\"images/imputation.png\" alt=\"drawing\" style=\"width:800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use an approach to that used above to impute missing values. For each missing values, identify the feature that is most correlated with the one containing the missing value. To avoid any spurious imputations, we will only use this method for pairs of features that have a correlation larger than 0.25. For example, if table able above did not contain `A`, then we would not be able to carry out the current methods given that the correlation between B and C  is less than 0.25.\n",
    "\n",
    "Use linear regression to impute all the missing values in features and the feature with the higher correlation with the condition that   $r>0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# bacteria.corr()\n",
    "# pH, oxygen, chloride, nitrates, CO2\n",
    "# pH: B1\n",
    "# oxygen: chloride\n",
    "# chloride: B1\n",
    "# nitrates: CO2\n",
    "# bacteria[bacteria.isna().any(1)]\n",
    "linreg = linregress(bacteria['B1'].iloc[bacteria['pH'].dropna().index], bacteria['pH'].dropna())\n",
    "bacteria['pH'] = bacteria['pH'].fillna(linreg.intercept + linreg.slope * bacteria['B1'])\n",
    "linreg = linregress(bacteria['B1'].iloc[bacteria['chloride'].dropna().index], bacteria['chloride'].dropna())\n",
    "bacteria['chloride'] = bacteria['chloride'].fillna(linreg.intercept + linreg.slope * bacteria['B1'])\n",
    "linreg = linregress(bacteria['chloride'].iloc[bacteria['oxygen'].dropna().index], bacteria['oxygen'].dropna())\n",
    "bacteria['oxygen'] = bacteria['oxygen'].fillna(linreg.intercept + linreg.slope * bacteria['chloride'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately, some features many not be correlated sufficiently with other features to use the imputation method above. We will use another approach, which leverages the nearest 4 nearest neighbors to fill in the missing values. Take for instance the following graph, with represents an instance in red (`e`) with a missing value and its three nearest neighbors (those are the blue vertices (`a`, `b` and `c`) that have edge with the `e` ). \n",
    "\n",
    "We can impute the missing valaue in `e` by taking the average of its three nearest neighbors (`a`, `b` and `b`).\n",
    "\n",
    "\n",
    "<img src=\"images/graph_imputation.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "- Use the Euclidean distance to compute the distnace between two instances.\n",
    "\n",
    "$$\n",
    "    d(s_1, s_2) = \\sqrt{\\sum_{i=1}^{p}(s_{1,i} - s_{2,i})^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "for na_index, na_row in bacteria[bacteria.isna().any(1)].iterrows():\n",
    "    distances = {}\n",
    "    for index, row in bacteria[bacteria.notna().all(1)].iterrows():\n",
    "        distance = ((pd.concat([na_row['season':'chloride'], na_row['B1':'B4']]) - pd.concat([row['season':'chloride'], row['B1':'B4']])) ** 2).sum() ** 0.5\n",
    "        if len(distances) < 4 or distance < min(distances.values()):\n",
    "            if len(distances) == 4:\n",
    "                distances.pop(max(distances, key=distances.get))\n",
    "            distances[index] = distance\n",
    "    bacteria.loc[na_index, 'nitrates'] = bacteria.loc[list(distances.keys()), 'nitrates'].mean()\n",
    "    bacteria.loc[na_index, 'CO2'] = bacteria.loc[list(distances.keys()), 'CO2'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python expression to show that your resulting data frame does not contain any missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the expression here\n",
    "bacteria[bacteria.isna().any(1)].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model: Train-Test Datasets\n",
    "\n",
    "* Split the original dataset into training testing sets.\n",
    "  * You can either do it manually or by using an appropriate `sklearn` library\n",
    "\n",
    "* Fit your data using the most adequate linear model; i.e., which degree polynomial provides the best results?\n",
    "  * Again, using your best judgment and your understanding of how the error is evaluated to asses the error obtained in each model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11.785269073141588)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(bacteria.loc[:, 'season':'CO2'], bacteria.loc[:, 'B1':'B4'])\n",
    "best_poly = 1\n",
    "min_error = float('inf')\n",
    "\n",
    "for i in range(0, 5):\n",
    "    poly = PolynomialFeatures(i)\n",
    "    train_transformed = poly.fit_transform(X_train)\n",
    "    test_transformed = poly.transform(X_test)\n",
    "    \n",
    "    lin = LinearRegression()\n",
    "    lin.fit(train_transformed, y_train)\n",
    "    y = lin.predict(test_transformed)\n",
    "    \n",
    "    error = np.sqrt(mean_squared_error(y_test, y))\n",
    "    if error < min_error:\n",
    "        best_poly = i\n",
    "        min_error = error\n",
    "        \n",
    "best_poly, min_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2-3 sentences, how do you model performs in generalizing to new instances?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your interpretation here\n",
    "It is not very precise. The result varies from degree 0 to degree 2, or from a constant function to a quadratic function, which is quite extreme. The error also varies quite a bit and is quite high, so it does not perform very well in generalizing to new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model: k-fold Cross-Validation\n",
    "\n",
    "* Repeat the above using k-fold Cross-Validation.\n",
    "  * I.e., find the best model and interpret your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12.520672197828542)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(bacteria.loc[:, 'season':'CO2'], bacteria.loc[:, 'B1':'B4'])\n",
    "best_poly = 1\n",
    "min_error = float('inf')\n",
    "\n",
    "for i in range(0, 5):\n",
    "    poly = PolynomialFeatures(i)\n",
    "    train_transformed = poly.fit_transform(X_train)\n",
    "    test_transformed = poly.transform(X_test)\n",
    "    \n",
    "    lin = LinearRegression()\n",
    "    \n",
    "    error = np.sqrt(np.mean(-cross_val_score(lin, train_transformed, y_train, scoring='neg_mean_squared_error', cv=10)))\n",
    "    if error < min_error:\n",
    "        best_poly = i\n",
    "        min_error = error\n",
    "        \n",
    "best_poly, min_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your interpretation here\n",
    "This method is much more precise. The result is consistently degree 1 and the error does not vary by much. However the error is still quite high, so it is still quite inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Models\n",
    "\n",
    "* Why do the models differ in their accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your answer here\n",
    "The methods vary in accuracy because first method is very much dependent on the split between the training and testing datasets. Once that is done, the model is trained only once on the training dataset before it is used to predict the testing dataset. However, the second method repeats itself for k iterations. During each iteration, the model is trained on the training dataset before being used to predict the validation dataset. The overall error is the average error across all iterations, so any bias resulting from a bad split is canceled out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
